{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT 3. BERT.ipynb","provenance":[],"authorship_tag":"ABX9TyP0L0GVXDrBw3N9TpJXTYBT"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"V3DFoORQcVFr"},"source":["# BERT\r\n","- BERT 모델의 개념과 학습 방식에 대해 살펴보겠습니다."]},{"cell_type":"markdown","metadata":{"id":"_UNpKFOvcXE8"},"source":["## BERT (**B**i-directional **E**ncoder **R**epresentations from **T**ransformers)\r\n","\r\n","- BERT는 bi-directional Transformer로 이루어진 언어 모델\r\n","- 잘 만들어진 BERT 언어 모델 위에 1개의 classification layer만 부착하여 다양한 NLP task를 수행\r\n","- 영어권에서 11개의 NLP task에 대해 sate-of-the-art(SOTA) 달성\r\n"]},{"cell_type":"markdown","metadata":{"id":"0uZX-w1pdey2"},"source":["## Methods - BERT 모델 학습 데이터\r\n","- 학습 코퍼스 데이터\r\n","  - BooksCorpus (800M words)\r\n","  - English Wikipedia (2,500M words without lists, tables and headers)\r\n","  - 30,000 token vocabulary\r\n","\r\n","- 데이터의 tokenizing\r\n","  - **WordPiece** tokenizing  \r\n","    He likes playing -> He likes play **##ing**\r\n","      - 빈도수가 많이 나오는 character 단위 인데 이런 걸 빈도수에 따라 분리하는 토크나이징 방법을 채택\r\n","  - 입력 문장을 tokenizing하고, 그 token들로 'token sequence'를 만들어 학습에 사용\r\n","  - 2개의 token sequence가 학습에 사용\r\n","\r\n"]}]}