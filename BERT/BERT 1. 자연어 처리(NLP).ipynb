{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bert 1. 자연어 처리(NLP).ipynb","provenance":[],"authorship_tag":"ABX9TyNkNyB2Qft0FOOHmEBzLfIl"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NzmLqanAnS3k"},"source":["# 01 자연어 처리 (Natural Language Processing, NLP)"]},{"cell_type":"markdown","metadata":{"id":"NJMmGYRipa90"},"source":["## 자연어 처리의 단계\r\n","- 전처리\r\n","  - 개행문자 제거\r\n","  - 특수문자 제거\r\n","  - 공백 제거\r\n","  - 중복 표현 제거 (ㅋㅋㅋㅋㅋ, ㅠㅠㅠㅠ, ...)\r\n","  - 이메일, 링크 제거\r\n","  - 제목 제거\r\n","  - 불용어(의미가 없는 용어) 제거\r\n","  - 조사 제거\r\n","  - 띄어쓰기, 문장분리 보정\r\n","  - 사전 구축\r\n","\r\n","- Tokenizing\r\n","- Lexical analysis\r\n","- Syntactic analysis\r\n","- Semantic analysis"]},{"cell_type":"markdown","metadata":{"id":"JIWwuCYgqE-z"},"source":["- 전처리\r\n","\r\n","- Tokenizing\r\n","  - 자연어를 어떤 단위로 살펴볼 것인가\r\n","  - 어절 tokenizing\r\n","  - 형태소 tokenizing\r\n","  - n-gram tokenizing\r\n","  - WordPiece tokenizing\r\n","\r\n","- Lexical analysis\r\n","  - 어휘 분석\r\n","  - 형태소 분석\r\n","  - 개체명 인식\r\n","  - 상호 참조\r\n","\r\n","- Syntactic analysis\r\n","  - 구문 분석\r\n","\r\n","- Semantic analysis\r\n","  - 의미 분석"]},{"cell_type":"markdown","metadata":{"id":"G2DoykKsqscT"},"source":["## 다양한 자연어 처리 Applications\r\n","- 문서 분류\r\n","- 문법, 오타 교정\r\n","- 정보 추출\r\n","- 음성 인식결과 보정\r\n","- 음성 합성 텍스트 보정\r\n","- 정보 검색\r\n","- 요약문 생성\r\n","- 기계 번역\r\n","- 질의 응답\r\n","- 기계 독해\r\n","- 챗봇\r\n","- 형태소 분석\r\n","- 개체명 분석\r\n","- 구문 분석\r\n","- 감성 분석\r\n","- 관계 추출\r\n","- 의도 파악"]},{"cell_type":"markdown","metadata":{"id":"ekbAeYtjrcoJ"},"source":["- Saltlux가 adams.ai라는 사이트에 자연어 처리 application 들을 오픈 API로 제공하고 있다.\r\n","\r\n","- 형태소 분석, 문서 분류, 개체명 인식 등, 대부분의 자연어 처리 문제는 '분류'의 문제"]},{"cell_type":"markdown","metadata":{"id":"S8R4T_oKsi3D"},"source":["## 특징 추출과 분류\r\n","\r\n","- '분류'를 위해선 데이터를 수학적으로 표현\r\n","- 먼저, **분류 대상의 특징(Feature)을 파악(Feature extraction)**\r\n","- 분류 대상의 특징(Feature)을 기준으로, 분류 대상을 **그래프 위에 표현** 가능\r\n","- 분류 대상들의 경계를 수학적으로 나눌 수 있음(Classification)\r\n","- 새로운 데이터 역시 특징을 기준으로 그래프에 표현하면, 어떤 그룹과 유사한지 파악 가능\r\n","\r\n","\r\n","\r\n","- 과거에는 사람이 직접 특징(Feature)을 파악해서 분류\r\n","- 실제 복잡한 문제들(ex. 자연어)에선 분류 대상의 특징을 사람이 파악하기 어려울 수 있음\r\n","- 이러한 특징을 컴퓨터가 스스로 찾고(Feature extraction), 스스로 분류(Classification)하는 것이 '기계학습'의 핵심 기술"]},{"cell_type":"markdown","metadata":{"id":"UkuGdfGet1sb"},"source":["## Word embedding - Word2Vec\r\n","\r\n","- 자연어를 어떻게 좌표평면 위에 표현할 수 있을까?\r\n","- 가장 단순한 표현 방법은 one-hot encoding 방식 -> Sparse representation\r\n","  - n개의 단어는 n차원의 벡터로 표현\r\n","  - But, 단어 벡터가 sparse해서 단어가 가지는 **'의미'**를 벡터 공간에 표현 불가능\r\n","\r\n","\r\n","- 그래서 **Word2Vec 라는 알고리즘이 발명**됨\r\n","- **Word2Vec(word to vector) 알고리즘** : 자연어(특히, 단어)의 의미를 벡터 공간에 임베딩\r\n","- **한 단어의 주변 단어들을 통해, 그 단어의 의미를 파악(주변 단어를 통해 그 단어의 의미를 임베딩 한다)**\r\n","  - 컴퓨터가 볼 때, 자연어는 우리가 보는 아랍어처럼 **'기호'**로만 보일 뿐!\r\n","  - 하지만 아랍어 주변의 단어가 비슷하다면, 그 두 아랍어가 무슨 뜻인지는 모르겠지만, 주변 단어 형태가 비슷하니 **의미**도 비슷할 것이다!\r\n","\r\n","- Word2vec은 간단한 신경망 구조로 이루어져 있음\r\n","- Word2vec 알고리즘은 주변부의 단어를 예측하는 방식으로 학습(Skip-gram 방식)\r\n","- 단어에 대한 dense vector를 얻을 수 있음\r\n","- Word2vec를 이용해서 단어의 의미가 벡터로 표현됨으로써 벡터 연산이 가능 \r\n","\r\n","****\r\n","\r\n","**< Word embedding의 방법론에 따른 특징 >**\r\n","1. Sparse representation\r\n","  - **One-hot encoding**\r\n","  - n개의 단어에 대한 n차원의 벡터\r\n","  - 단어가 커질수록 무한대 차원의 벡터가 생성\r\n","  - 주로 신경망의 입력단에 사용(신경망이 임베딩 과정을 대체. e.g. tf.layers.Embedding)\r\n","  - **의미 유추 불가능**\r\n","  - **차원의 저주(curse of dimensionality)**: 차원이 무한대로 커지면 정보 추출이 어려워짐\r\n","  - One-hot vector의 차원 축소를 통해 특징을 분류하고자 하는 접근도 있음 \r\n","\r\n","\r\n","2. Dense representation\r\n","  - **Word embedding**\r\n","  - **단어 vector를 한정된 차원으로 표현 가능**\r\n","  - **의미 관계 유추 가능**\r\n","  - 비지도 학습으로 단어의 의미 학습 가능\r\n","\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"Na6D69CNLiLl"},"source":["### Word embedding 성능 검증 방법\r\n","\r\n","https://github.com/dongjun-Lee/kor2vec,\r\n","https://github.com/SungjoonPark/KoreanWordVectors\r\n","\r\n","\r\n","\r\n","1. similarity(유사도) 계산\r\n","  - WordSim353\r\n","    - 영어권에서 만들어진 데이터\r\n","    - 13-16명의 사람이 annotate한 두 단어의 유사도\r\n","    - 두 단어 벡터의 cosine similarity를 구한 후 정답과 Spearman's rank-order correlation값을 획득\r\n","    - 경험상 0.7 이상의 값이 나오면 embedding이 잘 수행 됐음\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","2. Analogy test\r\n","  - 덧셈, 뺄셈을 이용해서 임베딩 공간을 확인하는 것\r\n","  a. Semantic analogy\r\n","    - 의미 관계를 파악하고자 테스트를 진행함\r\n","    - 대한민국 - 서울 + 도쿄    = 일본\r\n","  b. Syntactic analogy\r\n","    - 문법적인 구조도 임베딩이 됐는지 확인하는 것에 많이 사용\r\n","    - 밥 - 밥을 + 물을 = 물\r\n","\r\n","\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"Q3fEJl2dOXd0"},"source":["## Word embedding - Word2Vec 실습"]},{"cell_type":"code","metadata":{"id":"MD-3YoEntbXb"},"source":["# gensim 라이브러리 인스톨 먼저 하기\r\n","\r\n","from gensim.models.word2vec import Word2Vec\r\n","import gensim\r\n","\r\n","path = 'train_corpus.txt'\r\n","sentence = gensim.models.word2vec.Text8Corpus(path)\r\n","\r\n","model = Word2vec(sentence, min_count=5, size=100, window=5)\r\n","model.save('w2v_model')\r\n","\r\n","save_model = Word2Vec.load('w2v_model')\r\n","\r\n","word_vector = saved_model['강아지']\r\n","\r\n","saved_model.similarity('강아지', '멍멍이')\r\n","\r\n","saved_model.similar_by_word('강아지')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-N9UbmVvPLCM"},"source":["## Word2Vec 정리\r\n","- 단어가 가지는 의미 자체를 다차원 공간에 '벡터화'하는 것\r\n","- 중심 단어의 주변 단어들을 이용해 중심단어를 추론하는 방식으로 학습\r\n","\r\n","- 장점 \r\n","  - 단어간의 **유사도 측정**에 용이\r\n","  - 단어간의 **관계 파악**에 용이\r\n","  - 벡터 연산을 통한 **추론**이 가능 (e.g. 한국 - 서울 + 도쿄 =?)\r\n","\r\n","\r\n","- 단점\r\n","  - 단어의 **subword information** 무시 (e.g. **서울** vs **서울**시 vs 고양시)\r\n","    - '서울'이나, '서울시'나 **서울**이라는 subword가 일치하고, '서울시'와 '고양시'도 **시**라는 subword가 일치할 때, 사람은 단어에 있는 subword를 보고 의미파악을 할 수 있음. 하지만, **word2vec는 학습을 할 때 이것을 독립되어 있는 단어로 보기 때문에**, 서울시와 고양시가 유사한 의미를 가질 수 있도록 학습이 불가능 함.\r\n","\r\n","  - 그래서, Out of vocabulary (**OOV**)에서 적용 **불가능**\r\n","    - 학습에 사용되지 않은 단어가 들어오게 되면 그 단어에 대해서는 vector 공간을 추론할 수 없음"]},{"cell_type":"markdown","metadata":{"id":"pIZEMobfRsTX"},"source":["## Word embedding - FastText\r\n","\r\n","- 한국어는 다양한 용언 형태를 가짐\r\n","- Word2Vec의 경우, 다양한 용언 표현들이 서로 독립된 vocab으로 관리\r\n","  - 예) 동사 원형 : **모르**다\r\n","  - **모르**네, **모르**데, **모르**지, **모르**더라, **모르**겠으나, **몰라**, **몰랐**겠구나\r\n","  - 이렇게 용언이나, subword를 사용해서 임베딩 하면 더 좋은 임베딩이 이루어 지지 않을까? 해서 나온 것이 Fasttext\r\n","\r\n","\r\n","### Fasttext\r\n","- Facebook research에서 공개한 open source library (https://research.fb.com/fasttext/, fasttext.cc)\r\n","- C++11\r\n","- **gensim에 있어서** 가져다가 쓸 수 있음\r\n","- **단어를 *n*-gram으로 분리를 한 후, 모든 n-gram vector를 합산한 후 평균을 통해 단어 벡터를 획득**\r\n","- **오탈자 입력**에 대해서도 **본래 단어와 유사한 *n*-gram이 많아, 유사한 단어 벡터를 획득** 가능\r\n","\r\n","\r\n","\r\n","딥러닝에서 가장 큰 한계점은 OOV 문제가 많다는 것(학습에 사용되지 않은 입력이 들어왔을 때 정보처리가 불가능 하다)  \r\n","\r\n","하지만, Fasttext를 사용하면 OOV에 대해서도 어느정도 정보처리가 가능하기 때문에 많이 사용되고 있음\r\n","****\r\n","\r\n","- **Training**\r\n","  - 기존의 word2vec과 유사하나, 단어를 **n-gram**으로 나누어 학습을 수행\r\n","  - ***n-gram***의 범위가 **2-5**일 때, 단어를 다음과 같이 분리하여 학습함  \r\n","  \"**assumption**\" = (as, ss, su, ......, ass, ssu, ump, mpt, ......, ption, **assumption**)\r\n","  - 이 때, *n-gram*으로 나눠진 단어는 사전에 들어가지 않으며, 별도의 *n-gram vector*를 형성함\r\n","\r\n","\r\n","- **Testing**\r\n","  - 입력 단어가 vocabulary에 있을 경우, word2vec과 마찬가지로 해당 단어의 word vector를 return함\r\n","  - 만약 OOV일 경우, 입력 단어의 n-gram vector들의 합산을 return함"]},{"cell_type":"markdown","metadata":{"id":"Vs8mC6sSXevO"},"source":["## FastText와 Word2Vec의 비교\r\n","- 오탈자, OOV, 등장 회수가 적은 학습 단어에 대해서 강세\r\n","(https://link.springer.com/chapter/10.1007/978-3-030-12385-7_3)\r\n","\r\n","- Document frequency(DF)가 낮을 경우, word2vec은 유의미한 단어를 찾을 수 없음\r\n","- **Fasttext**의 경우, **subword information**이 유사한 단어를 찾았음"]},{"cell_type":"markdown","metadata":{"id":"u3g-W0UvZf5V"},"source":["## Word embedding - FastText 실습"]},{"cell_type":"code","metadata":{"id":"3jbUtixzZfoT"},"source":["# gensim 라이브러리 install 먼저 하기\r\n","\r\n","from gensim.models.word2vec import FastText\r\n","import gensim\r\n","\r\n","path = 'train_corpus.txt'\r\n","sentence = gensim.models.word2vec.Text8Corpus(path)\r\n","\r\n","model = FastText(sentence, min_count=5, size=100, window=5)\r\n","model.save('ft_model')\r\n","\r\n","save_model = FastText.load('ft_model')\r\n","\r\n","word_vector = saved_model['강아지']\r\n","\r\n","saved_model.similarity('강아지', '멍멍이')\r\n","\r\n","saved_model.similar_by_word('강아지') # similar_by_word를 통해 OOV 입력을 넣어도 gensim의 fasttext는 어떤 출력을 만들어 낼 수 있음"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_AFuW_eaAlS"},"source":["## Word embedding의 활용\r\n","\r\n","- 다른 자연어처리 모델의 입력으로 사용  \r\n","(e.g. 학습 데이터의 양이 매우 적은 감성분석을 수행할 경우, 학습 데이터만으로는 특성 추출 불가능)\r\n","  - 보통은 이 word embedding이 다른 자연어 처리의 가장 밑단에 사용이 된다. 딥러닝 네트워크가 됐든 어떤 기계학습 모델이 됐든, 어떤 feature을 input으로 넣어야지 분류를 해줄 것임. 그런데 이 feature을 word embedding을 이용해서 word2vector로 vector 공간을 feature로 사용하는 것\r\n","- 토픽 키워드(https://www.adams.ai/apiPage?deeptopicrank)"]},{"cell_type":"markdown","metadata":{"id":"I_0sF3tQcEuR"},"source":["## Word embedding 방식의 한계점\r\n","\r\n","- Word2Vec이나 FastText와 같은 word embedding 방식은 **동형어, 다의어 등에 대해선 embedding 성능이 좋지 못하다는 단점**이 있음\r\n","- 주변 단어를 통해 학습이 이루어지기 때문에, **'문맥'을 고려할 수 없음**"]}]}