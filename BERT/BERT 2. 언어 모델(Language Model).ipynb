{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT 2. 언어 모델(Language Model).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPaWFJvmUwpugNbSVgZ7YPm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WFwhjNAWg7YF"},"source":["# 언어 모델(Language Model, LM)\r\n","- 언어 모델의 개념과, BERT가 탄생하기 이전의 언어모델들을 살펴보겠습니다"]},{"cell_type":"markdown","metadata":{"id":"7jGnDCkAhEJM"},"source":["## 모델이란?\r\n","- 모형(model)\r\n","  - 어떤 상황이나 물체 등 연구 대상 주제를 도면이나 사진 등 화상을 사용하거나 수식이나 악보와 같은 기호를 사용하여 표현한 것\r\n","- 모델의 종류\r\n","  - 일기예보 모델, 데이터 모델, 비즈니스 모델, 물리 모델, 분자 모델 등\r\n","- 모델의 특징\r\n","  - **자연 법칙을 컴퓨터로 모사함**으로써 시뮬레이션이 가능\r\n","  - 이전 state를 기반으로 미래 state를 예측할 수 있음  \r\n","  (e.g. 습도와 바람 세기 등으로 내일 날씨 예측)\r\n","  - 즉, 미래의 state를 올바르게 예측하는 방식으로 모델 학습이 가능함\r\n"]},{"cell_type":"markdown","metadata":{"id":"m_oDxtk8iYtt"},"source":["## 언어 모델\r\n","- **'자연어'의 법칙을 컴퓨터로 모사**한 모델 -> 언어'**모델**'\r\n","- 주어진 단어들로부터 그 **다음에 등장한 단어의 확률을 예측**하는 방식으로 학습(이전 state로 미래 state를 예측)\r\n","- 다음의 등장할 단어를 잘 예측하는 모델은 그 **언어의 특성이 잘 반영된 모델**이자, **문맥**을 잘 계산하는 좋은 모델"]},{"cell_type":"markdown","metadata":{"id":"QItnIEcCjAH_"},"source":["그렇다면 언어 모델을 어떻게 만들어낼 수 있을까? \r\n","\r\n","\r\n","## Markov 확률 기반의 언어 모델\r\n","- 마코프 체인 모델(Markov Chain Model)\r\n","- 초기의 언어 모델은 다음의 단어나 문장이 나올 확률을 통계와 단어의 *n*-gram을 기반으로 계산\r\n","- 딥러닝 기반의 언어 모델은 **해당 확률을 최대로 하도록 네트워크를 학습**\r\n"]},{"cell_type":"markdown","metadata":{"id":"XS1vGKC4qBjh"},"source":["markov 모델은 굉장히 오래 되었고, 딥러닝으로 넘어 오면서 앞에 구조가 신경망 구조로 바뀌었다. 대표적인 신경망 구조가 RNN이다.\r\n","\r\n","\r\n","## Recurrent Naural Network(RNN) 기반의 언어 모델\r\n","- RNN은 **히든 노드가 방향을 가진 엣지로 연결돼 순환구조를 이루는(directed cycle) 인공신경망**의 한 종류\r\n","- **이전 state 정보가 다음 state를 예측**하는데 사용됨으로써, 시계열 데이터 처리에 특화 "]},{"cell_type":"markdown","metadata":{"id":"eNE2d1yPr6t_"},"source":["## RNN 언어 모델을 이용한 Application\r\n","- **마지막 출력**은 앞선 단어들의 **'문맥'**을 고려해서 만들어진 최종 출력 vector -> **Context vector**\r\n","- 출력된 context vector 값에 대해 classification layer를 붙이면 문장 분류를 위한 신경망 모델"]},{"cell_type":"markdown","metadata":{"id":"v3jpQ5OWvMOY"},"source":["## RNN 모델 기반의 Seq2Seq\r\n","- Seq2Seq(Sequence to Sequence) : 입력이 sequence로 들어가고 출력이 sequence로 나옴\r\n","- 입력이 문맥이 고려된 RNN 네트워크를 거치면 context vector가 나오고, 이 context vector를 다시 RNN으로 decoder을 하게 되면 vector에 대한 해석이 가능해짐.\r\n","- Encoder layer: RNN 구조를 통해 Context vector를 획득\r\n","- Decoder layer: Context vector를 입력으로 출력을 예측"]},{"cell_type":"markdown","metadata":{"id":"QST0yA5Iw01e"},"source":["## Encoder - Decoder\r\n","- RNN을 통해서 잘 encoding 된 언어를 다시 decoding을 하면?\r\n","  - 한국어로 encoding을 하고 영어로 decoding을 하면 번역엔진이 만들어짐"]},{"cell_type":"markdown","metadata":{"id":"R34yS2uBxJgq"},"source":["## Seq2Seq Applications\r\n","- Seq2Seq(Sequence to Sequence)\r\n","- Encoder layer: Context vector을 획득\r\n","- Decoder layer: 획득된 Context vector를 입력으로 다음 state를 예측"]},{"cell_type":"markdown","metadata":{"id":"_QsUIbJKx76l"},"source":["## RNN의 구조적 문제점\r\n","- 입력 **sequence의 길이가 매우 긴 경우**, **처음에 나온 token**에 대한 **정보가 희석**\r\n","- **고정된 context vector 사이즈**로 인해 긴 sequence에 대한 **정보를 함축하기 어려움**\r\n","- 모든 token이 영향을 미치니, **중요하지 않은 token도 영향**을 줌  \r\n","\r\n","\r\n","-> 그래서 만들어진 것이 바로 **Attention** 모델!!"]},{"cell_type":"markdown","metadata":{"id":"AKsjFIXGzquH"},"source":["## Attention 모델\r\n","- 인간이 정보처리를 할 때, 모든 sequence를 고려하면서 정보처리를 하는 것이 아님\r\n","- 인간의 정보처리와 마찬가지로, 중요한 feature는 더욱 중요하게 고려하는 것이 Attention의 모티브\r\n","\r\n","\r\n","- 기존 Seq2Seq에서는 RNN의 최종 output인 Context vector만을 활용\r\n","- Attention에서는 인코더 RNN 셀의 각각 output을 활용\r\n","- Decoder에서는 매 step마다 RNN 셀의 output을 이용해 dynamic하게 Context vector를 생성\r\n","\r\n","****\r\n","- 기존 RNN 모델에서 시작\r\n","- RNN 셀의 각 output들을 입력으로 하는 **Feed forward fully connected layer**을 거침\r\n","- 해당 **layer의 output을** **각 RNN 셀의 Score**로 결정\r\n","- 출력된 score에 **Softmax**를 취함으로써 **0-1 사이의 값**으로 변환\r\n","- 해당 값을 **Attention weight**로 결정\r\n","- **Attention weight와 hidden state를 곱해**서 **Context vector** 획득\r\n","- **Decoder의 hidden state가 attention weight 계산에 영향을 줌**\r\n","  - 만약 Decoder 결과가 정답과 많이 다르다면?\r\n","    - 좋지 못한 context vector -> 좋지 못한 attention weight\r\n","  - Query인 '난'과 'i', 'love', 'you'와의 상관관계 조정 필요.  \r\n","  Fully connected feed forward network에서 score를 조정(attention weight은 fully connected layer에서 만들어지기 때문)\r\n","  s1-s3(Soft max값)가 조정됨에 따라 Attention weight도 조정\r\n","***\r\n","#### 장점\r\n","- input에 대해서 어디가 중요한지 Attention weight로 학습이 가능함\r\n","  - Attention weight로 학습이 가능해지면 Attention이 어느 input과 output이 강하게 연결되어 있는지(어떤 feature들이 서로 강하게 연결되어 있는지) **시각화가 가능**해짐\r\n","  - 만약 내가 원하는대로 강하게 attention 연결이 안 되어 있다면? -> 모델이나 설계가 잘못된 것!(피드백을 받아서 고칠 수 있음)"]},{"cell_type":"markdown","metadata":{"id":"EmzhEtao933m"},"source":["## Attention 모델\r\n","- 문맥에 따라 동적으로 할당되는 encode의 **Attention weight**로 인한 **dynamic context vector를 획득**\r\n","- 기존 Seq2Seq의 **encoder, decoder 성능을 비약적으로 향상**시킴\r\n","***\r\n","#### 단점\r\n","- 하지만, **여전히 RNN이 순차적으로 연산이 이뤄짐**에 따라 **연산 속도가 느림**\r\n","  - 그냥 RNN을 없애는 건 어떨까?\r\n","    - 그래서 나온 것이 **Self-attention** 모델!!"]},{"cell_type":"markdown","metadata":{"id":"n139mM-C_Lk_"},"source":["## Self-attention 모델\r\n","- Attention is all you need!\r\n","- RNN을 encoder와 decoder에서 제거\r\n","\r\n","< self-attention의 탄생 >\r\n","- Decoder 결과가 정답과 많이 다르다면?  \r\n","좋지 못한 context vector -> 좋지 못한 attention weight  \r\n","Fully connected feed forward network에서 score를 조정  \r\n","s1-s3(Softmax값)가 조정됨에 따라 Attention weight도 조정   \r\n","\r\n","<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRB6c5FLCKLBrhqCWAUMuw8wx_YCyoVal5Sgw&usqp=CAU\" width=\"20\">\r\n","\r\n","- RNN + attention에 적용된 attention은 decoder가 해석하기에 가장 적합한 weight를 찾고자 노력\r\n","\r\n","<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRB6c5FLCKLBrhqCWAUMuw8wx_YCyoVal5Sgw&usqp=CAU\" width=\"20\">\r\n","\r\n","- Attention이 decoder가 아니라, input인 값을 가장 제대로 잘 표현할 수 있도록 학습하면?\r\n","  - -> 자기 자신을 가장 잘 표현할 수 있는 좋은 embedding\r\n","\r\n","<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRB6c5FLCKLBrhqCWAUMuw8wx_YCyoVal5Sgw&usqp=CAU\" width=\"20\">\r\n","\r\n","\r\n","- Self-attention 모델의 탄생!\r\n","****\r\n","목표: 자기 자신을 잘 표현하는 attention을 구하자!\r\n"]},{"cell_type":"markdown","metadata":{"id":"bdk-67PkDFUm"},"source":["## Self-attention 모델 단계\r\n","\r\n","1. 단어 임베딩\r\n","2. Concat\r\n","3. 3개의 weight vector 생성(학습 대상)\r\n","  - Query, Key, Value\r\n","    - Query : 얻고자 하는 context vector\r\n","    - Key : Query와의 상관관계를 나타내고자 하는 대상\r\n","    - value : Query와 Key의 상관관계 값\r\n","4. Input 문장의 token을 3 종류의 vector로 표현\r\n","5. 얻고자 하는 context vector인 Query * Key^T = Score\r\n","6. 이 Score을 Softmax를 취하면 **정수**값이 나옴  \r\n","-> **이 값**이 바로 **Self-attention의 Attention weight**이 됨\r\n","\r\n","7. Self-attention으로 나온 Softmax의 값 * Value\r\n","  - 'I' 처럼, 한 Query(내가 얻고자 하는 context vector)에 대해서 각각의 중요도가 곱셈으로 표현됨\r\n","8. 이렇게 곱셈으로 만들어진 모든 벡터를 합산하게 되면 Attention layer output 즉, **이 문장 속에서 단어 'I'가 지닌 의미!** 가 context vector로 표현됨\r\n","  - 이러한 과정을 전체를 대상으로 하게되면, 이 문장 속에서 'I'가 가지는 중요도가 context vector로 표현이 되고, 'study'가 가지는 중요도가 context vector로 표현이 되고, 'at'이 가지는 중요도가 context vector로 표현이 되고, 'school'이 가지는 중요도가 context vector로 표현이 됨. 이렇게 각각의 token에 대한 context vector를 독립적으로 구할 수 있음"]},{"cell_type":"markdown","metadata":{"id":"H2cQOev2I8s5"},"source":["## Multi-head Self Attention 모델\r\n","\r\n","- BERT에 사용\r\n","- Query, Key, Value로 구성된 attention layer를 동시에 여러 개를 수행하는 것\r\n","  - RNN 같은 경우 이전 state를 기다리고 다음 state를 예측하는 과정으로 만들어지지만, Self-attention은 모두 다 행렬 연산으로 표현됨\r\n","    - 즉, GPU를 사용하게 되면, Self-attention 여러 개를 넣어놔도 이 **행렬 연산**을 **병렬 처리**로 한 번에 할 수 있음\r\n","    - 실제로는 embedding을 더 복잡하게 만들어 내는데 **RNN보다 더 빨리 처리될 수 있음**\r\n","    - 실제로 BERT에서는 Multi-head Self Attention을 12개를 달아버림.  \r\n","    12개를 달아서 input 'I study at school'에 대한 학습을 진행하게 됨.\r\n","- 최종적으로 'I study at school'이 입력으로 들어갔을 때, 자기 자신을 표현하는 context vector를 합산을 통해 얻어낼 수 있음 \r\n","- 각각의 Self attention에서 나온 값들을 contat을 하고 -> feed forward network를 거쳐서 -> 최종 output이 나옴\r\n","  - 이렇게 **하나의 큰 과정**을 **Multi-head Self Attention의 Encoder 모델**이라고 함"]},{"cell_type":"markdown","metadata":{"id":"-kxHCjR7WypP"},"source":["Q. BERT에서 이 Multi-head Self Attention Encoder가 어떻게 적용되어 있을까?\r\n"," - A. BERT의 경우 더 복잡하게 되어있음. \r\n","\r\n","\r\n","\r\n"," ## Transformer 모델\r\n"," - Transformer 모델은 multi-head self attention으로 이루어진 encoder를 여러 층 쌓아서 encoding을 수행\r\n"," - Encoder layer1 -> Encoder layer2 -> Encoder layer3....를 거쳐 최종적인 context vector를 얻어냄\r\n","- BERT가 Cost가 많이 드는 이유?\r\n","  - Encoder layer 내부에는 multi-head로 연결이 되어있고 그 내부에 Self attention과 feed forward network이 연결되어 있음. 이것들이 모두 학습이 이루어져야 하다보니 학습이 오래 걸리고 데이터가 많이 필요함\r\n","\r\n","- 결국, **input에 대한 여러 encoder를 거쳐서 최종 encoder를 만들어 내는 것**. 이것이 **BERT에 적용된 Transformer 모델**임"]}]}