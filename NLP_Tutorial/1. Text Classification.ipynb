{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 텍스트 분류\n",
    "* Machine Learning, NLP: Text Classification using scikit-learn,python and NLTK.\n",
    "* Reference: [https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a](https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a)\n",
    "\n",
    "### Step\n",
    "1. Prerequisite and setting up the environment.\n",
    "2. Loading the data set in jupyter.\n",
    "3. Extracting features from text files.\n",
    "4. Running ML algorithms.\n",
    "5. Grid Search for parameter tuning.\n",
    "6. Useful tips and a touch of NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prerequisite and setting up the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Loading the data set in jupyter.\n",
    "* The data set will be using for this example is the famous **\"20 Newsgroup\"** data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "\n",
      "Usage\n",
      "~~~~~\n",
      "\n",
      "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "fetching / caching functions that downloads the data archive from\n",
      "the original `20 newsgroups website`_, extracts the archive contents\n",
      "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      ":func:`sklearn.datasets.load_files` on either the training or\n",
      "testing set folder, or both of them::\n",
      "\n",
      "  >>> from sklearn.datasets import fetch_20newsgroups\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "  >>> from pprint import pprint\n",
      "  >>> pprint(list(newsgroups_train.target_names))\n",
      "  ['alt.atheism',\n",
      "   'comp.graphics',\n",
      "   'comp.os.ms-windows.misc',\n",
      "   'comp.sys.ibm.pc.hardware',\n",
      "   'comp.sys.mac.hardware',\n",
      "   'comp.windows.x',\n",
      "   'misc.forsale',\n",
      "   'rec.autos',\n",
      "   'rec.motorcycles',\n",
      "   'rec.sport.baseball',\n",
      "   'rec.sport.hockey',\n",
      "   'sci.crypt',\n",
      "   'sci.electronics',\n",
      "   'sci.med',\n",
      "   'sci.space',\n",
      "   'soc.religion.christian',\n",
      "   'talk.politics.guns',\n",
      "   'talk.politics.mideast',\n",
      "   'talk.politics.misc',\n",
      "   'talk.religion.misc']\n",
      "\n",
      "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "attribute is the integer index of the category::\n",
      "\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "It is possible to load only a sub-selection of the categories by passing the\n",
      "list of the categories to load to the\n",
      ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "  >>> cats = ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "  >>> list(newsgroups_train.target_names)\n",
      "  ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      "Converting text to vectors\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "In order to feed predictive or clustering models with the text data,\n",
      "one first need to turn the text into vectors of numerical values suitable\n",
      "for statistical analysis. This can be achieved with the utilities of the\n",
      "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "example that extract `TF-IDF`_ vectors of unigram tokens\n",
      "from a subset of 20news::\n",
      "\n",
      "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "  ...               'comp.graphics', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectorizer = TfidfVectorizer()\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> vectors.shape\n",
      "  (2034, 34118)\n",
      "\n",
      "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "components by sample in a more than 30000-dimensional space\n",
      "(less than .5% non-zero features)::\n",
      "\n",
      "  >>> vectors.nnz / float(vectors.shape[0])       # doctest: +ELLIPSIS\n",
      "  159.01327...\n",
      "\n",
      ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \n",
      "returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
      ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
      "\n",
      "\n",
      "Filtering text for more realistic training\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "It is easy for a classifier to overfit on particular things that appear in the\n",
      "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "high F-scores, but their results would not generalize to other documents that\n",
      "aren't from this window of time.\n",
      "\n",
      "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "which is fast to train and achieves a decent F-score::\n",
      "\n",
      "  >>> from sklearn.naive_bayes import MultinomialNB\n",
      "  >>> from sklearn import metrics\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  # doctest: +ELLIPSIS\n",
      "  0.88213...\n",
      "\n",
      "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "the training and test data, instead of segmenting by time, and in that case\n",
      "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "yet of what's going on inside this classifier?)\n",
      "\n",
      "Let's take a look at what the most informative features are:\n",
      "\n",
      "  >>> import numpy as np\n",
      "  >>> def show_top10(classifier, vectorizer, categories):\n",
      "  ...     feature_names = np.asarray(vectorizer.get_feature_names())\n",
      "  ...     for i, category in enumerate(categories):\n",
      "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "  ...\n",
      "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "  alt.atheism: edu it and in you that is of to the\n",
      "  comp.graphics: edu in graphics it is for and of to the\n",
      "  sci.space: edu it that is in and space to of the\n",
      "  talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "You can now see many things that these features have overfit to:\n",
      "\n",
      "- Almost every group is distinguished by whether headers such as\n",
      "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "- Another significant feature involves whether the sender is affiliated with\n",
      "  a university, as indicated either by their headers or their signature.\n",
      "- The word \"article\" is a significant feature, based on how often people quote\n",
      "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "  wrote:\"\n",
      "- Other features match the names and e-mail addresses of particular people who\n",
      "  were posting at the time.\n",
      "\n",
      "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "barely have to identify topics from text at all, and they all perform at the\n",
      "same high level.\n",
      "\n",
      "For this reason, the functions that load 20 Newsgroups data provide a\n",
      "parameter called **remove**, telling it what kinds of information to strip out\n",
      "of each file. **remove** should be a tuple containing any subset of\n",
      "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "blocks, and quotation blocks respectively.\n",
      "\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')  # doctest: +ELLIPSIS\n",
      "  0.77310...\n",
      "\n",
      "This classifier lost over a lot of its F-score, just because we removed\n",
      "metadata that has little to do with topic classification.\n",
      "It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  # doctest: +ELLIPSIS\n",
      "  0.76995...\n",
      "\n",
      "Some other classifiers cope better with this harder version of the task. Try\n",
      "running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n",
      "the ``--filter`` option to compare the results.\n",
      "\n",
      ".. topic:: Recommendation\n",
      "\n",
      "  When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "  should strip newsgroup-related metadata. In scikit-learn, you can do this by\n",
      "  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "  lower because it is more realistic.\n",
      "\n",
      ".. topic:: Examples\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints all the categories\n",
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([7, 4, 4, ..., 3, 1, 8])\n",
      "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(twenty_train.target)\n",
    "pprint(np.unique(twenty_train.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n"
     ]
    }
   ],
   "source": [
    "# prints first line of the first data file\n",
    "print(\"\\n\".join(twenty_train.data[0].split('\\n')[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extracting features from text files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run machine learning algorithms we need to convert the text files into **numerical feature vectors**. We will be using **bag of words** model for our example.  \n",
    "Briefly, we segment each text file into words(for English splitting by space), and **count # of times each word occurs in each document** and finally **assign each word and integer id**. **Each unique word** in our dictionary will correspond to a **feature**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "Reference : [http://hleecaster.com/nlp-bag-of-words-concept/](http://hleecaster.com/nlp-bag-of-words-concept/)\n",
    "\n",
    "* 통계적 언어 모델 (SLM:Statistical Language Model)\n",
    "    - 통계적 언어 모델은 컴퓨터가 **\"확률 분포'**를 기반으로 언어를 이해하도록 하는 방법론  \n",
    "    \n",
    "    \n",
    "* 신경망 언어 모델 (NNLM: Neural Network Language Model)  \n",
    "\n",
    "\n",
    "* BoW (Bag-of-Words)\n",
    "    - **어휘의 빈도(개수)에 대해 통계적 언어 모델을 적용해서 나타낸 것**\n",
    "    - 등장하는 단어들의 숫자를 세서 그걸 가지고 무언가를 하는 것\n",
    "    - 말뭉치를 가방 안에 다 넣어서 단어 개수만 꺼내 살피는 방식이기 때문에 **단어의 순서를 무시한다**\n",
    "    - 여러 한계가 존재하지만, 비교적 훌륭한 성능을 보여주고 활용도가 높기 때문에 많이 쓰인다.\n",
    "    - 실제로 분석이나 활용을 할 때 **단어 사전**을 만들어놓은 후에, 문서(텍스트)가 있으면 그것을 벡터로 바꿔준다.(Vectorization)  \n",
    "    \n",
    "    \n",
    "* 문서-단어 행렬 (Document-Term matrix)\n",
    "    - 어떤 **문서에서 등장하는 각 단어들의 빈도를 나타낸 행렬**\n",
    "    - BoW와 별개의 개념이 아니라 BoW를 실제로 활용하기 위해 행렬의 형식으로 표현한 거라 생각하면 됨\n",
    "    - 보통 내가 가진 문서를 문서-단어 행렬로 놓는 게 자연어 처리 및 분석, 텍스트 마이닝의 진정한 시작점이 됨\n",
    "    - scikit-learn에서 **CounterVectorizer**을 활용하면 아주 쉽게 처리할 수 있음\n",
    "    \n",
    "\n",
    "* TF-IDF(Term Frequency-Inverse Document Frequency)\n",
    "    - 문서가 여러개 있을 때 단순히 **단어 빈도(Term Frequency)** 를 적용해서 문서-단어 행렬을 나타내면 문제가 있을 수도 있음\n",
    "    - **어떤 단어가 하나의 문서에서도 많이 사용되었다고 하더라도, 다른 모든 문서에서 널리 쓰이는 흔한 단어라면 이 단어는 특정성(specificity)이 떨어진다**\n",
    "    - 그래서 단순 단어 빈도로 접근하는 게 아니라, 어떤 단어가 한 문서에서 많이 나타난 동시에 다른 문서에서는 잘 나타나지 않는 것까지 고려하기 위한 개념이 등장. 이것이 바로 TF-IDF다.\n",
    "    - 즉, TF-IDF는 단순한 단어 빈도가 아니라 일종의 가중치를 적용한 개념이라고 이해하면 됨\n",
    "    - 그래서 이 TF-IDF를 활용해서 문서-단어 행렬을 만들고 분석을 하는 경우도 매우 많음\n",
    "    - scikit-learn에서는 아예 단순 빈도로 접근하는 CounterVectorizer말고, TF-IDF로 접근하는 **TfidfVectorizer** 클래스를 제공\n",
    "    \n",
    "    \n",
    "* n-gram\n",
    "    - BoW의 한계로 등장한 개념\n",
    "    - **단어를 n개씩 묶어서 그것을 하나의 feature(단어)로 보는 것**\n",
    "        - ex) \"이 음식은 너무 맛있다.\"\n",
    "            - BoW: \"이\", \"음식은\", \"너무\", \"맛있다\"\n",
    "            - bigram: \"이 음식은\", \"음식은 너무\", \"너무 맛있다\"\n",
    "        - BoW는 각 단어를 독립적으로 고려하지만 bigram, trigram과 같은 모델을 적용하면 단어가 나타나는 순서나 가까운 단어들을 함께 고려하게 되는 셈\n",
    "        \n",
    "   \n",
    "#### 결론\n",
    "- 아무리 ngram 모델을 적용하더라도 BoW 모델은 그 빈도만 세기 때문에 맥락을 충분히 고려해야 하는 상황, 즉 **텍스트를 생성한다거나 예측하는 등의 장면에는 활용이 어렵다.**\n",
    "- 게다가 학습된 단어 사전을 기반으로 하기 때문에 사전에 없는 새로운 단어가 나타났을 때 처리할 방법이 없다. 학습 데이터에 지나치게 의존하기 때문에 **오버피팅(overfitting)이 발생한다.** (사실 통계적 언어 모델에서 오버피팅이 나타나는 것은 당연하다)\n",
    "- **smoothing(평탄화)** 라는 방법을 통해 이 문제를 나름대로 어떻게든 해결하는 전략이 있긴하다.\n",
    "    - smoothing은 이미 알고 있는 단어들의 확률을 가지고, 알려지지 않은 단어의 확률을 추정하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 89 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we are learning the vocabulary dictionary and it returns a Documnet-Term matrix.  \n",
    "[n_samples, n_features]\n",
    "\n",
    "\n",
    "* **TF**: Just counting the number of words in each document has 1 issue: it will give more weightage to longer documents than shorter documents.  \n",
    "To avoid this, we can use frequency(TF-Term Frequencies) i.e.  \n",
    "\\# count(word) / # Total words, in each document.  \n",
    "* **TF-IDF**: reduce the weightage of more common words like 'the, is, an etc'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transoform a count matrix to a normalized tf or tf-idf representation\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)  # CounterVectorizer를 거친 data를 넣는다!\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Running ML algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a. **Naive Bayes**: [https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Building a pipeline:*** *We can write less code and do all of the above, by building a pipeline as follows:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "                     ])\n",
    "\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Performance of NB Classifier:*** *Now we will test the performance of the NB classifier on test set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.39 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "print(np.round(np.mean(predicted == twenty_test.target), 5)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7738980350504514"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{False: 1703, True: 5829}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(*np.unique((predicted == twenty_test.target), return_counts=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b. **Support Vector Machines(SVM)**: [https://scikit-learn.org/stable/modules/svm.html](https://scikit-learn.org/stable/modules/svm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to fit **a large-scale linear classifier without copying a dense numpy C-contiguous double precision array as input**, we suggest to use the **SGDClassifier(stochastic gradient descent)** class instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                        ('tfidf', TfidfTransformer()),\n",
    "                        ('clf-svm', SGDClassifier(loss='hinge',\n",
    "                                                 penalty='l2',\n",
    "                                                 alpha=1e-3,\n",
    "                                                 random_state=42)),\n",
    "                        ])\n",
    "\n",
    "_ = text_clf_svm.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.408 %\n"
     ]
    }
   ],
   "source": [
    "pred_svm = text_clf_svm.predict(twenty_test.data)\n",
    "print(np.round(np.mean(pred_svm == twenty_test.target), 5)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Grid Search for parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all the classifiers will have various parameters which can be tuned to obtain optimal performance. Scikit gives an extremely useful tll 'GridSearchCV'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal performance among various parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# All the parameters name start with the classifier name!\n",
    "parameters = {'vect__ngram_range':[(1, 1), (1, 2)],    # use unigram & bigrams and choose the one which is optimal.\n",
    "             'tfidf__use_idf':(True, False),\n",
    "             'clf__alpha': (1e-2, 1e-3),              # 1/100, 1/1000\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs = -1)  # n_jobs = -1 : to use multiple cores from user machine.\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.68 %\n",
      "{'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "# see the best mean score and the params\n",
    "\n",
    "print(np.round(gs_clf.best_score_ *100, 2), '%')\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Performance of NB Classifier after Grid Search***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_final = Pipeline([('vect', CountVectorizer(ngram_range = (1, 2))),\n",
    "                           ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "                           ('clf', MultinomialNB(alpha = 0.01))])\n",
    "text_clf_final = text_clf_final.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "pred_final = text_clf_final.predict(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.44 %\n"
     ]
    }
   ],
   "source": [
    "print(np.round(np.mean(pred_final == twenty_test.target), 4)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***77.39% => 83.44%***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b. Support Vector Machine(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "param_svm = {'vect__ngram_range': [(1,1), (1,2)],\n",
    "            'tfidf__use_idf': (True, False),\n",
    "            'clf-svm__alpha': (1e-2, 1e-3),\n",
    "            }\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, param_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.01 %\n",
      "{'clf-svm__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "print(np.around(gs_clf_svm.best_score_ * 100, 2), '%')\n",
    "print(gs_clf_svm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Performance of SVM Classifier after Grid Search***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_svm_final = Pipeline([('vect', CountVectorizer(ngram_range = (1,2))),\n",
    "                              ('tfidf', TfidfTransformer(use_idf =True)),\n",
    "                              ('clf-svm', SGDClassifier(loss = 'hinge',\n",
    "                                                       penalty = 'l2',\n",
    "                                                       alpha = 1e-3,\n",
    "                                                       random_state = 42))])\n",
    "text_clf_svm_final = text_clf_svm_final.fit(twenty_train.data, twenty_train.target)\n",
    "pred_svm_final = text_clf_svm_final.predict(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.5 %\n"
     ]
    }
   ],
   "source": [
    "print(np.around(np.mean(pred_svm_final == twenty_test.target), 3)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***82.408 % => 83.5%***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Useful tips and a touch of NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)  **Removing** : (the, then etc) from the data. In most of the text classification problems, this is indeed not useful. Update the code for creating object of CountVectorizer as follows;\n",
    "\n",
    "**Let's see if removing stop words increases the accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultinomialNB())])\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.69 %\n"
     ]
    }
   ],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "pred = text_clf.predict(twenty_test.data)\n",
    "print(np.round(np.mean(pred == twenty_test.target), 4)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***77.39% => 81.69 %***\n",
    "\n",
    "You can try the same for SVM and also while doing grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal performance among various parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# parameters name start with the classifier name!\n",
    "param = {'vect__ngram_range':[(1, 1), (1, 2)],   # unigram & bigrams which is optimal\n",
    "         'tfidf__use_idf':(True, False),\n",
    "         'clf__alpha': (1e-2, 1e-3)}             # 1/100, 1/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "gs_clf = GridSearchCV(text_clf, param, n_jobs = -1)      # n_jobs = -1 : to use multiple cores from user machine.\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.58 %\n",
      "{'clf__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "# see the best mean score and the params\n",
    "\n",
    "print(np.round(gs_clf.best_score_ * 100, 2), '%')\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Performance of NB Classifier after Gfrid Search***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_clf_final = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range = (1, 2))),\n",
    "#                            ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "#                            ('clf', MultinomialNB(alpha = 0.01))])\n",
    "# text_clf_final = text_clf_final.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "# pred_final = text_clf_final.predict(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.006 %\n"
     ]
    }
   ],
   "source": [
    "# print(np.round(np.mean(pred_final == twenty_test.target), 5)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_final = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range = (1, 1))),\n",
    "                           ('tfidf', TfidfTransformer(use_idf = False)),\n",
    "                           ('clf', MultinomialNB(alpha = 0.01))])\n",
    "text_clf_final = text_clf_final.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "pred_final = text_clf_final.predict(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.134 %\n"
     ]
    }
   ],
   "source": [
    "print(np.round(np.mean(pred_final == twenty_test.target), 5)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***81.69% => 84.134%***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b. Support Vector Machine(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                              ('tfidf', TfidfTransformer()),\n",
    "                              ('clf-svm', SGDClassifier(loss = 'hinge',\n",
    "                                                       penalty = 'l2',\n",
    "                                                       alpha = 1e-3,\n",
    "                                                       random_state = 42))])\n",
    "_ = text_clf_svm.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.262 %\n"
     ]
    }
   ],
   "source": [
    "pred_svm = text_clf_svm.predict(twenty_test.data)\n",
    "print(np.round(np.mean(pred_svm == twenty_test.target), 5)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***82.408% => 82.262%***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "param_svm = {'vect__ngram_range':[(1, 1), (1, 2)],\n",
    "             'tfidf__use_idf':(True, False),\n",
    "             'clf-svm__alpha':(1e-2, 1e-3)}\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, param_svm, n_jobs = -1)\n",
    "gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.61 %\n",
      "{'clf-svm__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "print(np.around(gs_clf_svm.best_score_ * 100, 2), '%')\n",
    "print(gs_clf_svm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Performance of SVM Classifier after Grid Search***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_svm_final = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range = (1,2))),\n",
    "                              ('tfidf', TfidfTransformer(use_idf =True)),\n",
    "                              ('clf-svm', SGDClassifier(loss = 'hinge',\n",
    "                                                       penalty = 'l2',\n",
    "                                                       alpha = 1e-3,\n",
    "                                                       random_state = 42))])\n",
    "text_clf_svm_final = text_clf_svm_final.fit(twenty_train.data, twenty_train.target)\n",
    "pred_svm_final = text_clf_svm_final.predict(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.2 %\n"
     ]
    }
   ],
   "source": [
    "print(np.round(np.mean(pred_svm_final == twenty_test.target), 3)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***82.408% => 83.2%***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) **FitPrior = False** : When set to false for MultinomialNB, a uniform prior will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultinomialNB(fit_prior = False))])\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.14 %\n"
     ]
    }
   ],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "pred = text_clf.predict(twenty_test.data)\n",
    "print(np.round(np.mean(pred == twenty_test.target), 4)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***81.69% => 82.14%***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) **Stemming** : stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form. (ex. A stemming algorithm reduces the words \"fishing\", \"fished\", and \"fisher\" to the root word, \"fish\".)\n",
    "\n",
    "We need NLTK which comes with various stemmers that can help reducing the words to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py\", line 2106, in run\n",
      "    for msg in self.data_server.incr_download(self.items):\n",
      "  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py\", line 623, in incr_download\n",
      "    for msg in self._download_list(info_or_id, download_dir, force):\n",
      "  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py\", line 669, in _download_list\n",
      "    for msg in self.incr_download(item, download_dir, force):\n",
      "  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py\", line 637, in incr_download\n",
      "    for msg in self.incr_download(info.children, download_dir, force):\n",
      "  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py\", line 623, in incr_download\n",
      "    for msg in self._download_list(info_or_id, download_dir, force):\n",
      "  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py\", line 669, in _download_list\n",
      "    for msg in self.incr_download(item, download_dir, force):\n",
      "  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py\", line 643, in incr_download\n",
      "    for msg in self._download_package(info, download_dir, force):\n",
      "  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py\", line 737, in _download_package\n",
      "    for msg in _unzip_iter(filepath, zipdir, verbose=False):\n",
      "  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py\", line 2252, in _unzip_iter\n",
      "    zf.extractall(root)\n",
      "  File \"C:\\Users\\user\\Anaconda3\\lib\\zipfile.py\", line 1636, in extractall\n",
      "    self._extract_member(zipinfo, path, pwd)\n",
      "  File \"C:\\Users\\user\\Anaconda3\\lib\\zipfile.py\", line 1691, in _extract_member\n",
      "    shutil.copyfileobj(source, target)\n",
      "  File \"C:\\Users\\user\\Anaconda3\\lib\\shutil.py\", line 82, in copyfileobj\n",
      "    fdst.write(buf)\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Snowball stemmer works very well for English language\n",
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVctorizer) :\n",
    "    def build_analyzer(self) :\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_count_vect = StemmedCounVectorizer(stop_words='english')\n",
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect),\n",
    "                            ('tfidf', TfidfTransoformer()),\n",
    "                            ('mnb', MultinomialNB(fit_prior = Fasle))])\n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(twenty_test.data)\n",
    "print(np.round(np.mean(predicted_mnb_stemmed == twenty_test.target)*100,3), '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
